model_config:
  visual_bert:
    bert_model_name: bert-base-uncased
    training_head_type: pretraining
    visual_embedding_dim: 2048
    special_visual_initialize: true
    embedding_strategy: plain
    bypass_transformer: false
    output_attentions: false
    output_hidden_states: false
    random_initialize: false
    freeze_base: false
    finetune_lr_multiplier: 1
    # Default points to BERT pooler strategy which is to take
    # representation of CLS token after passing it through a dense layer
    pooler_strategy: default
    zerobias: false     # Initialize last layer to predict closer to 0 on init for sigmoid outputs
    
    text_processor:
      type: glove
      params:
         max_length: 128
         vocab:
           type: intersected
           embedding_name: glove.6B.300d
           vocab_file: ${env.data_dir}/datasets/vqa2/defaults/extras/vocabs/vocabulary_100k.txt
         preprocessor:
           type: simple_sentence
           params: {}